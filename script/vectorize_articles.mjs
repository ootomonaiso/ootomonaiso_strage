import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import { glob } from 'glob';
import matter from 'gray-matter';
import crypto from 'crypto';
import { createClient } from '@supabase/supabase-js';
import { pipeline } from '@xenova/transformers';

// __dirname for ESM
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Supabase setup
const supabaseUrl = process.env.SUPABASE_URL;
const supabaseKey = process.env.SUPABASE_KEY;
const supabase = createClient(supabaseUrl, supabaseKey);

// BERT embedder init
const embedder = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');

// SHA256 hash
function computeHash(text) {
  return crypto.createHash('sha256').update(text).digest('hex');
}

// ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆBERTï¼‰
async function getEmbedding(text) {
  const output = await embedder(text, {
    pooling: 'mean',
    normalize: true,
  });

  console.log('ðŸ“¦ Raw output from embedder:', output);

  if (!output || !output.data || !output.data.length) {
    throw new Error('âŒ Invalid embedding tensor structure');
  }

  return Array.from(output.data); // Float32Array â†’ JS Array
}

// Markdown å‡¦ç†
async function processMarkdownFiles() {
  const files = glob.sync('../pro/**/*.{md,mdx}', {
    ignore: ['../pro/node_modules/**']
  });

  console.log('ðŸ“„ Matched Markdown files:', files);
  const processedFiles = [];

  for (const filePath of files) {
    try {
      const fullPath = path.resolve(filePath);
      const relativePath = path.relative(path.resolve(__dirname, '..'), fullPath);
      const raw = fs.readFileSync(fullPath, 'utf8');
      const { data: meta, content } = matter(raw);
      const cleanedContent = content.trim();
      const hash = computeHash(cleanedContent);

      const { data: existing, error } = await supabase
        .from('documents')
        .select('hash')
        .eq('file_path', relativePath)
        .single();

      const needsInsert = error || !existing || existing.hash !== hash;

      if (needsInsert) {
        const embeddingArray = await getEmbedding(cleanedContent);
        const embedding = `[${embeddingArray.map(v => Number(v.toFixed(8))).join(', ')}]`;

        const payload = {
          file_path: relativePath,
          meta,
          content: cleanedContent,
          embedding,
          hash,
        };

        const { error: upsertError } = await supabase
          .from('documents')
          .upsert(payload, { onConflict: 'file_path' });

        if (upsertError) {
          console.error(`âŒ DB upsert error for ${relativePath}:`, upsertError);
        } else {
          console.log(`âœ… Processed file: ${relativePath}`);
        }
      } else {
        console.log(`ðŸ” Skipping unchanged file: ${relativePath}`);
      }

      processedFiles.push(relativePath);
    } catch (err) {
      console.error(`ðŸ”¥ Error processing file: ${filePath}`, err);
    }
  }

  const { data: dbFiles } = await supabase.from('documents').select('file_path');
  const dbFilePaths = dbFiles.map((item) => item.file_path);
  const toDelete = dbFilePaths.filter((dbPath) => !processedFiles.includes(dbPath));

  for (const delPath of toDelete) {
    const { error: delError } = await supabase
      .from('documents')
      .delete()
      .eq('file_path', delPath);
    if (delError) {
      console.error(`âŒ Failed to delete ${delPath}:`, delError);
    } else {
      console.log(`ðŸ—‘ï¸ Deleted record for file: ${delPath}`);
    }
  }
}

processMarkdownFiles()
  .then(() => console.log('ðŸŽ‰ All Markdown files processed'))
  .catch((err) => console.error('ðŸ’¥ Error processing Markdown files:', err));
